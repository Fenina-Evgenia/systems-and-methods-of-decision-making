# systems-and-methods-of-decision-making

# Метрические алгоритмы классификации

Метрический классификатор - алгоритм классификации, основанный на вычислении оценок сходства между объектами.

Гипотеза компактности: cхожим объектам соответствуют схожие ответы.

**_Мерой близости_** называют функцию расстояния ![](http://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%3A%20%28X%20%5Ctimes%20X%29%20%5Crightarrow%20%5Cmathbb%7BR%7D). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга.

Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект *u* к тому классу *y*, для которого **суммарный вес ближайших обучающих объектов ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) максимален**:
![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%20%3A%20y_%7Bu%7D%5E%7B%28i%29%7D%20%3D%20y%7D%20w%28i%2C%20u%29%20%5Crightarrow%20max)
, где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.

Функция ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) называется **_оценкой близости объекта u к классу y_**. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.

К метрическим методам классификации относятся:
- KNN - алгоритм k ближайших соседей
- KwNN - алгоритм k взвешенных ближайших соседей
- PW - метод парзеновского окна
- PF - метод	потенциальных	функций
- STOLP - алгоритм отбора	эталонных	объектов

## Алгоритм k ближайших соседей (kNN)

Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера).
Данный алгоритм классификации относит классифицируемый объект *u* к тому классу *y*, к которому относится большинство из *k* его ближайших соседей *x(u_i)*.

Для оценки близости классифицируемого объекта *u* к классу *y* **алгоритм kNN** использует следующую функцию:

![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D) , где *i* - порядок соседа по расстоянию к классифицируемому объекту *u*.

Реализация алгоритма: [kNN](knn.R)

Как выбрать *k*:

При *k = 1* получим метод ближайшего соседа и, соответственно, неустойчивость к шуму

При *k = l* наоборот, алгоритм чрезмерно устойчив и вырождается в констатну. 

Таким образом, крайние значения *k* нежелательны. На практике оптимальное *k* подбирается по критерию скользящего контроля [LOO](LOOknn.R)

Иллюстрация к проекту:

![alt text](https://github.com/Fenina-Evgenia/systems-and-methods-of-decision-making/blob/master/kNN%26LOO.png)


### Преимущества:
1. Простота реализации.

2. При **k** (оптимальном), алгоритм "неплохо" классифицирует.

### Недостатки:
1. Нужно хранить всю выборку.

2. При **k = 1** неустойчивость к погрешностям , выброс(погрешность) классифицируется неверно окружающие его объекты.

3. При **k = l** алгоритм вырождается в константу.

4. Малый набор параметров.

5. Точки с одинаковым расстоянием вызывают неопределенность 


## Метод парзеновского окна (pw)

Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l*. В данном алгоритме весовая функция *w_i* определяется как функция **от расстояния между классифицируемым объектом *u* и его соседями *x(u_i), i = 1, ..., l*, а не от ранга соседа *i***, как было в весовом kNN.

Для оценки близости классифицируемого объекта *u* к классу *y* **метод парзеновского окна** использует следующую функцию:

![](https://latex.codecogs.com/gif.latex?W%28i%2C%20u%29%20%3D%20K%5Cleft%20%28%20%5Cfrac%7B%5Crho%20%28u%2C%20x_%7Bu%7D%5E%7Bi%7D%29%7D%7Bh%7D%20%5Cright%20%29) , где *K(z)* - функция ядра (не возрастающая от 0 до бесконечности), а *h* - ширина окна (окно - сферическая окрестность классифицируемого объекта *u* радиуса *h*, при попадании в которую обучающий объект *xi* "голосует" за отнесение объекта *u* к классу *yi*).

Реализация алгоритма: [PW](PW.R)

Иллюстрация к проекту:

![alt text](https://github.com/Fenina-Evgenia/systems-and-methods-of-decision-making/blob/master/PW.png)

### Преимущества:
1. Простота реализации.
2. Все точки, попадающие в окно, расстояние между которыми одинаково, будут учитываться (в отличие от алгоритма *взвешенного kNN*).
3. Скорость классификации быстрее, т.к. не требуется сортировка расстояний (*O(l)*).
4. Окно с переменной шириной решает проблему разрешимости задач, в которых обучающие выборки распределены неравномерно по пространству *X* (как ирисы Фишера).
5. Выбор финитного ядра позволяет свести классификацию объекта *u* к поиску *k* его ближайших соседей, тогда как при не финитном ядре (например, гауссовском) нужно перебирать всю обучающую выборку *Xl*, что может приводить к неприемлемым затратам времени (при большом *l*).

### Недостатки:
1. Слишком узкие окна приводят к неустойчивой классификации, а слишком широкие - к вырождению алгоритма в константу.
2. Диапазон, из которого выбирается параметр *h*, нужно подбирать самим.
3. "Скудный" набор параметров.
4. Если в окно, радиуса *h*, не попало ни одной точки *x_i*, то алгоритм не способен классифицировать объект *u*.
5. Если суммарные веса классов оказываются одинаковыми, то алгоритм относит классифицируемый объект *u* к любому из классов.

### Метод потенциальных функции (PF)

Для оценки близости объекта _u_ к классу _y_ алгоритм использует следующую функцию:

![](http://latex.codecogs.com/svg.latex?W_y%28i%2C%20u%29%20%3D%20%5Cgamma_i%20%5Ccdot%20K%28%5Cfrac%7B%5Crho%28u%2C%20x_u%5Ei%29%7D%7Bh_i%7D%29%2C%20%5Cgamma_i%20%5Cgeq%200%2C%20h_i%20%3E%200)
, где 
![](http://latex.codecogs.com/svg.latex?%5Clarge%20K%28z%29) — функция ядра.

В программной реализации применяется квартическое ядро.
Алгоритм подбирает только силу потенциала
![](http://latex.codecogs.com/svg.latex?%5Cgamma_i), радиусы потенциалов
_h_ известны заранее.

__Простыми словами:__ алгоритм для каждого обучающего объекта _x_ строит
окружность, радиуса _h_ и силы воздействия (потенциала)
![](http://latex.codecogs.com/svg.latex?%5Cgamma_i).

# Байесовские алгоритмы классификации
**Байесовские алгоритмы классификации** основаны на предположении, что есть вероятностное пространство ![](https://latex.codecogs.com/gif.latex?X%20%5Ctimes%20Y) с неизвестной плотностью распределения ![](https://latex.codecogs.com/gif.latex?%5Crho%20%28x%2C%20y%29%20%3D%20P%28y%29%5Crho%20%28x%20%7C%20y%29), из которого случайно и независимо извлекаются *l* наблюдений.

Байесовский подход опирается на теорему о том, что **если плотности распределения классов известны, то алгоритм классификации, имеющий минимальную вероятность ошибок, можно выписать в явном виде**.

Обозначим *величину потери* алгоритмом *а* при неправильной классификации объекта класса *y* ![](https://latex.codecogs.com/gif.latex?%5Clambda%20_%7By%7D).

**Теорема:** Если известны априорные вероятности классов *P(y)* и функции правдоподобия *p(x*|*y)*, то минимум среднего риска достигается алгоритмом ![](https://latex.codecogs.com/gif.latex?a%28x%29%20%3D%20arg%20%5Cmax_%7By%20%5Cepsilon%20Y%7D%5Clambda%20_%7By%7DP%28y%29%5Crho%28x%7Cy%29). Алгоритм *a(x)* называется **оптимальным байесовским решающим правилом**.

На практике зачастую плотности распределения классов неизвестны и их приходится восстанавливать по обучающей выборке. **Чем лучше удастся восстановить функции правдоподобия, тем ближе к оптимальному будет построенный алгоритм**.

В зависимости от способов восстановления плотности существует большое разнообразие **байесовских алгоритмов классификации**.

## "Наивный" байесовский классификатор (nb)

Предполагается, что все объекты обучающей выборки *X* описываются *n* числовыми признаками ![](https://latex.codecogs.com/gif.latex?f_j%2C%20j%20%3D%201%2C%20...%2C%20n). Все эти признаки -- независимые случайные величины. Тогда функции правдоподобия классов представимы в виде:

![](https://latex.codecogs.com/gif.latex?%5Crho_y%28x%29%20%3D%20%5Crho_%7By_%7B1%7D%7D%28f_1%28x%29%29%5Ccdot%20...%20%5Ccdot%20%5Crho_%7By_%7Bn%7D%7D%28f_n%28x%29%29%2C%20y%20%5Cepsilon%20Y) ,

где ![](https://latex.codecogs.com/gif.latex?%5Crho_%7By_%7Bj%7D%7D%28f_j%28x%29%29) -- плотность распределения значений *j*-го признака для класса *y*.

Данный алгоритм основан на предположении, что *n* одномерных плотностей оценить проще, чем одну *n*-мерную, но на практике это редко получается.

Эмпирическая оценка *n*-мерной плотности находится следующим образом:

![](https://latex.codecogs.com/gif.latex?%5Chat%7Bp%7D_h%28x%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%20%3D%201%7D%5E%7Bm%7D%20%5Cprod_%7Bj%20%3D%201%7D%5E%7Bn%7D%20%5Cfrac%7B1%7D%7Bh_j%7D%20K%20%5Cleft%28%20%5Cfrac%7Bf_j%28x%29%20-%20f_j%28x_i%29%29%29%7D%7Bh_j%7D%20%5Cright%20%29) ,

где *x* -- классифицируемая точка, *x_i* -- точки обучающей выборки, *h* -- ширина окна, *m* -- кол-во точек выборки, *n* -- кол-во признаков, *K* -- функция ядра, *f_j* -- признаки.

Подставив эту оценку в оптимальное байесовское решающее правило, получим **"наивный" байесовский классификатор**.

В качестве обучающей выборки *X* рассматриваются ирисы Фишера, ядро *K* выбрано Гауссовское.

Реализация алгоритма: [NBC](NBC.R)

Иллюстрация к проекту:

![alt text](https://github.com/Fenina-Evgenia/systems-and-methods-of-decision-making/blob/master/NBC.png)

Основные __преимущества__ наивного байесовского классификатора:
- Простота реализации.
- Низкие вычислительные затраты при обучении и классификации.
- В тех редких случаях, когда признаки действительно независимы (или почти независимы), наивный байесовский классификатор (почти) оптимален.

Основной его __недостаток__ — относительно низкое качество классификации в большинстве реальных задач.
